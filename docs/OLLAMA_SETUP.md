# ğŸ¤– Local LLM Setup with Docker Ollama

This directory contains everything needed to run local LLM models alongside your MCP server using Docker Ollama.

## ğŸš€ Quick Start

### 1. Start Ollama Service

**With GPU (Recommended):**
```bash
./scripts/manage-ollama.sh start-gpu
```

**CPU Only:**
```bash
./scripts/manage-ollama.sh start-cpu
```

### 2. Download Models
```bash
./scripts/manage-ollama.sh setup
```

This will download efficient models:
- **llama3.2:3b** - Fast general purpose (2GB)
- **codellama:7b** - Specialized for coding (4GB)
- **mistral:7b** - Good for analysis (4GB)

### 3. Test the Service
```bash
./scripts/manage-ollama.sh test
```

## ğŸ“Š Model Information

| Model | Size | Use Case | Memory |
|-------|------|----------|---------|
| llama3.2:3b | 2GB | General chat, quick responses | 4GB RAM |
| codellama:7b | 4GB | Code generation, debugging | 8GB RAM |
| mistral:7b | 4GB | Analysis, reasoning | 8GB RAM |

## ğŸ”§ Management Commands

```bash
# Service Management
./scripts/manage-ollama.sh start-gpu     # Start with GPU
./scripts/manage-ollama.sh start-cpu     # Start CPU-only
./scripts/manage-ollama.sh stop          # Stop service
./scripts/manage-ollama.sh restart       # Restart service
./scripts/manage-ollama.sh status        # Check status

# Model Management
./scripts/manage-ollama.sh models        # List models
./scripts/manage-ollama.sh pull llama3.1:8b  # Pull specific model
./scripts/manage-ollama.sh setup         # Setup default models

# Monitoring
./scripts/manage-ollama.sh logs          # Show logs
./scripts/manage-ollama.sh test          # Test functionality

# Maintenance
./scripts/manage-ollama.sh clean         # Remove all data
```

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   MCP Server    â”‚ â”€â”€ â”‚   LLM Gateway   â”‚ â”€â”€ â”‚  Docker Ollama  â”‚
â”‚ (Port 4000)     â”‚    â”‚                 â”‚    â”‚ (Port 11434)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚                       â”‚
         â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚  Cursor LLM     â”‚              â”‚
                        â”‚   (Fallback)    â”‚              â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
                                                          â”‚
                                                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                                â”‚  Local Models   â”‚
                                                â”‚  (Persistent)   â”‚
                                                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ”§ Configuration

### GPU Requirements
- NVIDIA GPU with 6GB+ VRAM (recommended)
- CUDA drivers installed
- nvidia-docker2 installed

### CPU Requirements
- 8GB+ RAM (16GB recommended)
- Modern multi-core CPU

### Storage Requirements
- 10GB+ free space for models
- SSD recommended for better performance

## ğŸš¨ Troubleshooting

### Service won't start
```bash
# Check Docker
docker --version
docker compose version

# Check system resources
./scripts/manage-ollama.sh status
docker compose logs
```

### Models won't download
```bash
# Check internet connection
curl -I https://ollama.ai

# Check disk space
df -h

# Try manual download
./scripts/manage-ollama.sh pull llama3.2:3b
```

### Slow performance
```bash
# Check if GPU is being used
nvidia-smi  # Should show ollama process

# Check memory usage
docker stats

# Try smaller models
./scripts/manage-ollama.sh pull phi3:mini
```

## ğŸ”— Integration with MCP Server

The LLM Gateway automatically detects and uses local Ollama models:

1. **Primary**: Local Ollama models (fastest, private)
2. **Fallback**: Cursor built-in LLM (when local unavailable)

Test integration:
```bash
# Start MCP server (in another terminal)
poetry run python protocol_server.py

# Test with coordinator
poetry run python -c "
from src.agents.coordinator.coordinator_integration import process_user_message_with_memory
import asyncio
response = asyncio.run(process_user_message_with_memory('Hello, test local LLM'))
print(response)
"
```

## ğŸ“ File Structure

```
cursor/
â”œâ”€â”€ docker-compose.yml          # Docker services
â”œâ”€â”€ .env.ollama                 # Configuration
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ manage-ollama.sh        # Management script
â”‚   â””â”€â”€ setup-models.sh         # Model setup
â””â”€â”€ docs/
    â””â”€â”€ OLLAMA_SETUP.md         # This file
```

## ğŸ¯ Next Steps

1. **Start the service**: `./scripts/manage-ollama.sh start-gpu`
2. **Setup models**: `./scripts/manage-ollama.sh setup`
3. **Test integration**: `./scripts/manage-ollama.sh test`
4. **Start your MCP server**: `poetry run python protocol_server.py`
5. **Use with Cursor**: Chat with coordinator using local LLMs!

## ğŸ”’ Privacy & Security

- **âœ… Fully local**: No data sent to external services
- **âœ… Persistent**: Models stay downloaded
- **âœ… Isolated**: Runs in Docker containers
- **âœ… Secure**: No external network access required

Your code and conversations stay completely private! ğŸ”
