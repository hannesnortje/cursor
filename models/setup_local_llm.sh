#!/bin/bash
# Setup script for local LLM environment

echo "ğŸ¤– AI Agent System - LLM Setup"
echo "================================"

# Check if Ollama is installed
if ! command -v ollama &> /dev/null; then
    echo "âŒ Ollama not found. Installing..."
    curl -fsSL https://ollama.ai/install.sh | sh
else
    echo "âœ… Ollama is installed at $(which ollama)"
fi

# Check if Ollama service is running
if curl -s http://localhost:11434/api/tags > /dev/null 2>&1; then
    echo "âœ… Ollama service is running"
else
    echo "ğŸ”„ Starting Ollama service..."
    ollama serve &
    sleep 5
fi

# Check if llama3.1:8b is available
if ollama list | grep -q "llama3.1:8b"; then
    echo "âœ… llama3.1:8b model is available"
else
    echo "ğŸ“¥ Downloading llama3.1:8b model..."
    ollama pull llama3.1:8b
fi

# Test the model
echo "ğŸ§ª Testing LLM connection..."
if curl -s -X POST http://localhost:11434/api/generate \
    -H "Content-Type: application/json" \
    -d '{
        "model": "llama3.1:8b",
        "prompt": "Hello",
        "stream": false
    }' > /dev/null; then
    echo "âœ… LLM is responding correctly"
else
    echo "âŒ LLM test failed"
fi

echo ""
echo "ğŸ“Š Current setup:"
ollama list

echo ""
echo "ğŸ¯ Configuration:"
echo "  â€¢ LLM Server: http://localhost:11434"
echo "  â€¢ Primary Model: llama3.1:8b"
echo "  â€¢ Model Storage: System managed by Ollama"
echo "  â€¢ Your Code: /media/hannesn/storage/Code/cursor"
echo ""
echo "âœ… Local LLM setup complete!"
