# Ollama Docker Configuration
# Copy this to .env and modify as needed

# Ollama service configuration
OLLAMA_HOST=0.0.0.0:11434
OLLAMA_PORT=11434

# Memory limits (adjust based on your system)
# For GPU systems with 8GB+ VRAM, you can increase these
OLLAMA_MAX_LOADED_MODELS=3
OLLAMA_NUM_PARALLEL=4
OLLAMA_MAX_QUEUE=512

# Performance settings
# Set to false if you have limited memory
OLLAMA_FLASH_ATTENTION=true
OLLAMA_KV_CACHE_TYPE=f16

# GPU settings (for NVIDIA GPUs)
# Uncomment and adjust if you have multiple GPUs
# CUDA_VISIBLE_DEVICES=0

# Model storage settings
# The models will be stored in Docker volume 'ollama_data'
# This persists across container restarts
